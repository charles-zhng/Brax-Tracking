name: train_fly_freejnt
version: ${resolve_default:debug,${..version}}
gpu: ${resolve_default:0,${..gpu}}
wandb_project: "eabe_debug"
env_name: fly_freejnt_clip
algo_name: ppo
task_name: ${dataset.dname}
note: 
num_envs: 1024 
num_timesteps: 5_000_000_000
eval_every: 100_000_000
episode_length: 601
batch_size: ${train.num_envs}
learning_rate: 1e-4
num_minibatches: 32
num_updates_per_batch: 8
clipping_epsilon: 0.2
entropy_cost: 1e-2
action_repeat: 1
unroll_length: 20
discounting: 0.95
mlp_policy_layer_sizes: [512, 512]
restore_checkpoint: ''
state_metric_list: 
  - pos_reward
  - quat_reward
  - joint_reward
  - angvel_reward
  - bodypos_reward
  - endeff_reward
  - reward_ctrl
  - healthy_reward
  - too_far
  - bad_pose
  - bad_quat
  - fall
info_metric_list: 
  - start_frame
  - summed_pos_distance
  - current_frame
  - pos_distance
  - quat_distance
  - joint_distance
  - angvel_distance
  - bodypos_distance
  - endeff_distance